import tensorflow as tf
from tensorflow import keras
from tensorflow import Tensor
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Add, ReLU, Input, AveragePooling2D, LeakyReLU, Conv2DTranspose

import numpy as np
import matplotlib.pyplot as plt

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # Restrict TensorFlow to only allocate 2GB of memory on the first GPU
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Virtual devices must be set before GPUs have been initialized
    print(e)

data = "moz"
mfcc_shape = 39
length = 192
classes = 2

X_train = np.load(f'mfccs/X_train_{data}.npy').reshape(-1, mfcc_shape, length, 1)
X_test = np.load(f'mfccs/X_test_{data}.npy').reshape(-1, mfcc_shape, length, 1)
X_val = np.load(f'mfccs/X_val_{data}.npy').reshape(-1, mfcc_shape, length, 1)
y_train = np.load(f'mfccs/y_train_{data}.npy')
y_test = np.load(f'mfccs/y_test_{data}.npy')
y_val = np.load(f'mfccs/y_val_{data}.npy')

print(X_train.shape, X_test.shape, X_val.shape, len(y_train), len(y_test), len(y_val))

y_train_hot = to_categorical(y_train, num_classes=classes)
y_test_hot = to_categorical(y_test, num_classes=classes)
y_val_hot = to_categorical(y_val, num_classes=classes)

callbacks = [TensorBoard(log_dir='./logs')]

def residual_stack(input, filters):
    """Convolutional residual stack with weight normalization.

    Args:
        filter: int, determines filter size for the residual stack.

    Returns:
        Residual stack output.
    """
    c1 = Conv2D(filters, 3, dilation_rate=1, padding="same")(input)
    lrelu1 = LeakyReLU()(c1)
    c2 = Conv2D(filters, 3, dilation_rate=1, padding="same")(lrelu1)
    add1 = Add()([c2, input])

    lrelu2 = LeakyReLU()(add1)
    c3 = Conv2D(filters, 3, dilation_rate=3, padding="same")(lrelu2)
    lrelu3 = LeakyReLU()(c3)
    c4 = Conv2D(filters, 3, dilation_rate=1, padding="same")(lrelu3)
    add2 = Add()([add1, c4])

    lrelu4 = LeakyReLU()(add2)
    c5 = Conv2D(filters, 3, dilation_rate=9, padding="same")(lrelu4)
    lrelu5 = LeakyReLU()(c5)
    c6 = Conv2D(filters, 3, dilation_rate=1, padding="same")(lrelu5)
    add3 = Add()([c6, add2])

    return add3


def conv_block(input, conv_dim, upsampling_factor):
    """Dilated Convolutional Block with weight normalization.

    Args:
        conv_dim: int, determines filter size for the block.
        upsampling_factor: int, scale for upsampling.

    Returns:
        Dilated convolution block.
    """
    conv_t = Conv2DTranspose(conv_dim, 16, upsampling_factor, padding="same")(input)
    lrelu1 = LeakyReLU()(conv_t)
    res_stack = residual_stack(lrelu1, conv_dim)
    lrelu2 = LeakyReLU()(res_stack)
    return lrelu2


def discriminator_block(input):
    conv1 = Conv2D(16, 15, 1, "same")(input)
    lrelu1 = LeakyReLU()(conv1)
    conv2 = Conv2D(64, 41, 4, "same", groups=4)(lrelu1)
    lrelu2 = LeakyReLU()(conv2)
    conv3 = Conv2D(256, 41, 4, "same", groups=16)(lrelu2)
    lrelu3 = LeakyReLU()(conv3)
    conv4 = Conv2D(1024, 41, 4, "same", groups=64)(lrelu3)
    lrelu4 = LeakyReLU()(conv4)
    conv5 = Conv2D(1024, 41, 4, "same", groups=256)(lrelu4)
    lrelu5 = LeakyReLU()(conv5)
    conv6 = Conv2D(1024, 5, 1, "same")(lrelu5)
    lrelu6 = LeakyReLU()(conv6)
    conv7 = Conv2D(1, 3, 1, "same")(lrelu6)
    return [lrelu1, lrelu2, lrelu3, lrelu4, lrelu5, lrelu6, conv7]


def create_generator(input_shape):
    inp = keras.Input(input_shape)
    x = MelSpec()(inp)
    x = Conv2D(512, 7, padding="same")(x)
    x = LeakyReLU()(x)
    x = conv_block(x, 256, 8)
    x = conv_block(x, 128, 8)
    x = conv_block(x, 64, 2)
    x = conv_block(x, 32, 2)
    x = Conv2D(1, 7, padding="same", activation="tanh")(x)
    return keras.Model(inp, x)


def create_discriminator(input_shape):
    inp = keras.Input(input_shape)
    out_map1 = discriminator_block(inp)
    pool1 = AveragePooling2D()(inp)
    out_map2 = discriminator_block(pool1)
    pool2 = AveragePooling2D()(pool1)
    out_map3 = discriminator_block(pool2)
    return keras.Model(inp, [out_map1, out_map2, out_map3])


generator = create_generator((None, 1))
discriminator = create_discriminator((None, 1))



print(X_train.shape, y_train_hot.shape, X_val.shape, y_val_hot.shape)
print(model.summary())
history = model.fit(X_train, y_train_hot, batch_size=64, epochs=10, verbose=1,
            validation_data=(X_val, y_val_hot), callbacks=callbacks)


training_loss = history.history['loss']
test_loss = history.history['val_loss']

training_acc = history.history['accuracy']
test_acc = history.history['val_accuracy']

# Create count of the number of epochs
epoch_count = range(1, len(training_acc) + 1)

# Visualize loss history

plt.subplots(1, 2)
plt.subplot(1, 2, 1)
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Test Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.plot(epoch_count, training_acc, 'r--')
plt.plot(epoch_count, test_acc, 'b-')
plt.legend(['Training Accuracy', 'Test Accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.tight_layout()
plt.savefig("cnn_resblock.png")
plt.show()

# what really optimized my model: smaller learning rate, larger number of epochs,
#
# model.save('final_tert_model.h5')
print(model.summary())
